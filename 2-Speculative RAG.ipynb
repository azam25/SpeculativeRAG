{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c856e6-5823-4b31-abe4-6fc28e1f0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import concurrent.futures\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"YOUR_BASE_URL\n",
    "os.environ[\"OTEL_SDK_DISABLED\"] = \"true\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "OpenAI.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "OpenAI.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# Function to load and process documents\n",
    "def load_and_process_document(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    db_instructEmbedd = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever_model = db_instructEmbedd.as_retriever(search_kwargs={\"k\": 5})\n",
    "    print(\"Embedding has been generated successfully.\")\n",
    "    return retriever_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e866fdc0-60a3-457c-916a-387adaefc125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding has been generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load retriever model\n",
    "file_path = \"Data/20230728_RFI_ZEISS+AMS_Support+Provider_Cloud+Integrator.pdf\"\n",
    "retriever = load_and_process_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a62a4e7-aa72-47a9-9551-8f47221f320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speculative Generation Function\n",
    "def speculative_generate(question):\n",
    "    speculative_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a fast and lightweight assistant. Explain the question in detailed easy langauge with assumptions so retrieval can be easy and accurate. You response should have only explained question without any other explaination or details\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\"\n",
    "        },\n",
    "    ]\n",
    "    speculative_response = client.chat.completions.create(\n",
    "        model=\"Mixtral-8x7B-Instruct-v0.1-TDU\",\n",
    "        messages=speculative_prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    return speculative_response.choices[0].message.content\n",
    "\n",
    "# Context Retrieval Function\n",
    "def retrieve_context(question, retriever):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    if not docs:\n",
    "        return \"\"\n",
    "    context = \"Context:\\n\" + \"\\n\".join([f\"Chunk {i + 1}: {doc.page_content}\" for i, doc in enumerate(docs[:3])])\n",
    "    return context\n",
    "\n",
    "# Final Answer Generation\n",
    "def generate_final_answer(question, context):\n",
    "    final_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert in analyzing RFP documents. Use the provided context to generate a complete and detailed answer. \"\n",
    "                       \"If the context does not contain information relevant to the question, respond with 'No Information available'. \"\n",
    "                       \"Ensure the final answer includes at least 7 bullet points when possible.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Context: {context}\\n\\nQuestion: {question}\\n\\nGenerate the Final Answer in bullet points under the heading 'Final Answer from LLM:'.\"\"\"\n",
    "        },\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Mixtral-8x7B-Instruct-v0.1-TDU\",\n",
    "        messages=final_prompt,\n",
    "        temperature=0.0,\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36bd7119-8107-473e-b082-c6e0160347e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speculative RAG Pipeline\n",
    "def speculative_rag_pipeline(file_path, question):\n",
    "    # Step 1: Perform speculative generation and context retrieval in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        speculative_future = executor.submit(speculative_generate, question)    \n",
    "        speculative_answer = speculative_future.result()\n",
    "        print(\"speculative_future *************** :\", speculative_answer)\n",
    "        context_future = executor.submit(retrieve_context, question, retriever)\n",
    "        context = context_future.result()\n",
    "        print(\"context_future *************** :\", context)\n",
    "\n",
    "    # Step 2: Validate speculative answer against context\n",
    "    if \"No Information available\" in speculative_answer or not context:\n",
    "        print(\"Speculative answer or context is insufficient. Generating answer directly from context.\")\n",
    "        return generate_final_answer(question, context)\n",
    "\n",
    "    # Step 3: Generate the final answer\n",
    "    final_answer = generate_final_answer(question, context)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d3b17d6-d799-408d-8312-0a5c152359cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speculative_future *************** : \n",
      "Prompt: Create a clear and concise question that encapsulates the following task: Analyze the 'Background' and 'Objectives' sections of a Request for Proposal (RFP) to determine the main challenges faced by the customer. Ensure the question is focused on identifying specific customer needs such as efficiency improvements, technology upgrades, quality enhancement, sustainability, innovation, and compliance.\n",
      "\n",
      "Question: Based on the 'Background' and 'Objectives' sections of the RFP, what are the specific challenges faced by the customer, and what technology or domain-related improvements, enhancements, or innovations are required to address these needs, as indicated by keywords such as 'efficiency improvements', 'technology up\n",
      "context_future *************** : Context:\n",
      "Chunk 1: Carl Zeiss AG \n",
      " \n",
      "Request for Information –  \n",
      "Application Management and Support Provider  \n",
      "(Cloud Integrator) \n",
      " \n",
      "   \n",
      "  \n",
      "   \n",
      "28.07.2023  Page 13 \n",
      " \n",
      "ZEISS classification: Restricted \n",
      "ZEISS classification: Restricted \n",
      "Section 4 – Detailed expectation listing \n",
      "This section contains an overview of the expected components of your proposal which shall be \n",
      "responded to . All documentation shall be provided in English.  ZEISS will conduct th e following \n",
      "process steps (5-phase approach): \n",
      "• RFI (long list) \n",
      "• RFP (short list) \n",
      "• Down select and final negotiations \n",
      "• T&T \n",
      "• Run- Mode \n",
      " \n",
      "Should you be selected for the RFP phase you will be considered as a potential tactical Service \n",
      "Provider in the full ZEISS CIT Multi Provider landscape. This classification of a partnership requires \n",
      "a clear classification in the ZEISS ITSM Ecosystem and its various service providers in the \n",
      "overarching set of rules and regulations.\n",
      "Chunk 2: CIT Corporate IT \n",
      "Cloud \n",
      "Integrator  \n",
      "Definition in relevance to the RFI request: \n",
      "The Cloud Integrator connects various cloud environments, systems  and applications from \n",
      "vendors so that they are fully integrated into the ZEISS IT Service Management and support \n",
      "processes (the so-called ZEISS ITSM Ecosystem). This ensures the interaction of all parties \n",
      "involved in such a way that, in the end, every user within ZEISS is provided with the best \n",
      "possible integrated services based on the ZEISS ITSM Ecosystem.  \n",
      "FMO Future Mode of Operation \n",
      "FWA Framework Agreement \n",
      "GSD Global Service Desk \n",
      "HW Hardware \n",
      "KPI Key Performance Indicator \n",
      "LSD Local Service Desk \n",
      "MIM Major Incident Management \n",
      "PI Performance Indicator \n",
      "Q&A Question and Answer \n",
      "RFI Request for Information \n",
      "RFP Request for Proposal \n",
      "SoW Statement of Work \n",
      "SoW SM Statement of Work Service Management \n",
      "SW Software \n",
      "ZEISS Carl Zeiss AG and subordinated affiliates\n",
      "Chunk 3: a clear classification in the ZEISS ITSM Ecosystem and its various service providers in the \n",
      "overarching set of rules and regulations. \n",
      " \n",
      "For this purpose, ZEISS would already like to give an outlook on the procedure in the RFP phase. \n",
      "ZEISS will then discuss three key contract documents and their annexes together.  \n",
      "• - FWA (default document for all our service providers) \n",
      "• - SoW Service Management (default document for all our service providers)  \n",
      "• - SoW Service (will be individually designed and discussed per service provider) \n",
      " \n",
      "Overview Contract structure:\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "**Final Answer from LLM:**\n",
      "\n",
      "Based on the provided RFP document, the following problem statement can be drafted:\n",
      "\n",
      "1. The current IT landscape at Carl Zeiss AG is facing challenges in integrating various cloud environments, systems, and applications from different vendors into a unified ZEISS IT Service Management Ecosystem.\n",
      "\n",
      "2. There is a need for a clear classification of a partnership with a tactical Service Provider in the ZEISS CIT Multi Provider landscape, adhering to the overarching set of rules and regulations.\n",
      "\n",
      "3. Inefficiencies may arise due to the lack of a streamlined process for connecting and managing multiple cloud environments, leading to potential service disruptions and user dissatisfaction.\n",
      "\n",
      "4. The need for improved service management is indicated by the requirement for detailed contract documents, such as FWA, SoW Service Management, and SoW Service, to ensure seamless integration and operation of services.\n",
      "\n",
      "5. Executive management is seeking innovation and sustainability in the IT infrastructure, necessitating technology upgrades and efficiency improvements.\n",
      "\n",
      "6. Compliance is a concern, as the RFP mentions the need for a partnership that can adhere to ZEISS's classification and service provider regulations within the ZEISS ITSM Ecosystem.\n",
      "\n",
      "7. To address these challenges, Carl Zeiss AG requires a Cloud Integrator that can effectively connect various cloud environments, systems, and applications while ensuring compliance, innovation, and sustainability in their IT infrastructure.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "question = \"\"\"\n",
    "your task is to draft a succinct problem statement reflecting the challenges outlined in the RFP, including technology and domain-specific work.\n",
    " Instructions: \n",
    "  1. Review the RFP's 'Background' and 'Objectives' sections to identify the customer's key challenges.\n",
    "  2. Look for keywords indicating customer needs, such as 'efficiency improvements', 'technology upgrades', 'quality enhancement', 'Executive summary','sustainability', 'innovation', and 'compliance'.\n",
    "\"\"\"\n",
    "final_answer = speculative_rag_pipeline(file_path, question)\n",
    "\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c1358-d9e6-4c40-b804-38df11bcade3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a97b7d-c0e1-4fd3-8018-d6931203cfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b9f26-058a-43be-bd6d-d2ef1cc16874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd79cb-77fb-42bc-9b52-94b6b540ba18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "173515d0-4b45-4982-af1c-f59556035a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Environment Setup (Replace with your credentials)\n",
    "# -----------------------------------------------------\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"jMR2UEXpYEDHDSEhPB64HFyX0apBWONC\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://llm-server.llmhub.t-systems.net/v2\"\n",
    "os.environ[\"OTEL_SDK_DISABLED\"] = \"true\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. Load & Process Document (your functions)\n",
    "# -------------------------------------------\n",
    "def load_and_process_document(file_path):\n",
    "    \"\"\"\n",
    "    Loads a PDF file, splits it into chunks, \n",
    "    creates embeddings, and returns a FAISS retriever.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    db_instructEmbedd = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever_model = db_instructEmbedd.as_retriever(search_kwargs={\"k\": 5})\n",
    "    print(\"Embedding has been generated successfully.\")\n",
    "    return retriever_model\n",
    "\n",
    "def retrieve_context(question, retriever):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant document chunks for the given question,\n",
    "    and formats them into a context string.\n",
    "    \"\"\"\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    if not docs:\n",
    "        return \"Context:\\nNo relevant information found.\"\n",
    "\n",
    "    # Take top 3 chunks for brevity\n",
    "    context = \"Context:\\n\" + \"\\n\".join([\n",
    "        f\"Chunk {i + 1}: {doc.page_content}\" for i, doc in enumerate(docs[:3])\n",
    "    ])\n",
    "    return context\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. Define helper functions for Speculative RAG\n",
    "# -----------------------------------------\n",
    "\n",
    "def generate_draft(question, context, small_model=\"gpt-35-turbo\"):\n",
    "    \"\"\"\n",
    "    Generate a draft answer using a smaller/faster model.\n",
    "    \"\"\"\n",
    "    draft_prompt = f\"\"\"\n",
    "You are a helpful AI assistant that tries to produce a concise first draft.\n",
    "The user has asked the following question: \"{question}\"\n",
    "\n",
    "Here is some context that may be useful:\n",
    "{context}\n",
    "\n",
    "Please provide a brief draft answer below:\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=small_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": draft_prompt}\n",
    "        ],\n",
    "        temperature=0  # Keep temperature low for factual consistency\n",
    "    )\n",
    "\n",
    "    draft_answer = response.choices[0].message.content.strip()\n",
    "    print('Speculative generations : ',draft_answer + '\\n\\n')\n",
    "    return draft_answer\n",
    "\n",
    "def refine_answer(question, context, draft, large_model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Refine the draft answer using a larger/more capable model.\n",
    "    \"\"\"\n",
    "    refine_prompt = f\"\"\"\n",
    "        You are a knowledgeable AI assistant tasked with refining a draft answer.\n",
    "        \n",
    "        User's Question:\n",
    "        \"{question}\"\n",
    "        \n",
    "        Context (useful information):\n",
    "        {context}\n",
    "        \n",
    "        Draft Answer:\n",
    "        {draft}\n",
    "        \n",
    "        Please refine the draft so it is factually correct, clear, and concise. \n",
    "        If the draft is already good, just confirm it. Otherwise, improve or correct it. Format in heading and sub heading for better readabiity.  \n",
    "        Final refined answer: \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=large_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": refine_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    final_answer = response.choices[0].message.content.strip()\n",
    "    return final_answer\n",
    "\n",
    "def speculative_rag_answer(question, retriever):\n",
    "    \"\"\"\n",
    "    Orchestrates the Speculative RAG process:\n",
    "    1) Retrieve context \n",
    "    2) Draft answer with smaller model\n",
    "    3) Refine answer with larger model\n",
    "    \"\"\"\n",
    "    # 1) Retrieve the context\n",
    "    context = retrieve_context(question, retriever)\n",
    "\n",
    "    # 2) Draft answer (small/faster model)\n",
    "    draft_response = generate_draft(question, context)\n",
    "\n",
    "    # 3) Refine answer (larger/more capable model)\n",
    "    final_response = refine_answer(question, context, draft_response)\n",
    "\n",
    "    return final_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07244a55-0223-47e1-a65e-a8fe0d011293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding has been generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1) Load and process your PDF (adjust file path as needed)\n",
    "file_path = \"Data/Large Language Model based Multi-Agents.pdf\"\n",
    "retriever = load_and_process_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0da892a-cfdc-4266-a7ef-597d730f8e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculative generations :  Single-Agent Systems are systems that are powered by LLMs and are designed to tackle complex tasks by breaking them down into smaller subgoals. These systems are focused on formulating their internal mechanisms and interactions with the external environment. In contrast, LLM-MA systems emphasize diverse agent profiles, inter-agent interactions, and collective decision-making processes. While single-agent systems have achieved considerable progress in complex problem-solving and decision-making, LLM-based multi-agent systems have shown even more inspiring cognitive abilities.\n",
      "\n",
      "\n",
      "Final Refined Answer:\n",
      " ### Single-Agent Systems\n",
      "\n",
      "Single-Agent Systems are systems powered by Large Language Models (LLMs) designed to tackle complex tasks by breaking them down into smaller subgoals. These systems focus on formulating their internal mechanisms and interactions with the external environment. \n",
      "\n",
      "#### Key Features:\n",
      "- **Decision-Making Thought**: LLM-based agents can methodically think through each part of a task, sometimes exploring multiple paths, and learn from past experiences to improve decision-making.\n",
      "- **Task Decomposition**: They break down complex tasks into manageable subgoals, allowing for systematic problem-solving.\n",
      "\n",
      "In contrast, LLM-based Multi-Agent (LLM-MA) systems emphasize diverse agent profiles, inter-agent interactions, and collective decision-making processes. While single-agent systems have made significant progress in complex problem-solving and decision-making, LLM-MA systems have demonstrated even more inspiring cognitive abilities through collaboration and communication among multiple autonomous agents.\n",
      "\n",
      "This refined answer provides a clear and concise explanation of Single-Agent Systems, highlighting their key features and contrasting them with Multi-Agent Systems.\n"
     ]
    }
   ],
   "source": [
    "# 2) Ask a question\n",
    "user_question = \"Explain Single-Agent Systems\"\n",
    "\n",
    "# 3) Get the final answer via Speculative RAG\n",
    "answer = speculative_rag_answer(user_question, retriever)\n",
    "\n",
    "print(\"Final Refined Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69ba0e-8c6e-40b6-ace9-35c0de11cc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
