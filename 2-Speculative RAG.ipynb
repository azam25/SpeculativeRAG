{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c856e6-5823-4b31-abe4-6fc28e1f0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import concurrent.futures\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"YOUR_BASE_URL\n",
    "os.environ[\"OTEL_SDK_DISABLED\"] = \"true\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "OpenAI.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "OpenAI.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# Function to load and process documents\n",
    "def load_and_process_document(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    db_instructEmbedd = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    retriever_model = db_instructEmbedd.as_retriever(search_kwargs={\"k\": 5})\n",
    "    print(\"Embedding has been generated successfully.\")\n",
    "    return retriever_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e866fdc0-60a3-457c-916a-387adaefc125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding has been generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load retriever model\n",
    "file_path = \"YOUR_FILE_PATH\n",
    "retriever = load_and_process_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a62a4e7-aa72-47a9-9551-8f47221f320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speculative Generation Function\n",
    "def speculative_generate(question):\n",
    "    speculative_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a fast and lightweight assistant. Explain the question in detailed easy langauge with assumptions so retrieval can be easy and accurate. You response should have only explained question without any other explaination or details\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\"\n",
    "        },\n",
    "    ]\n",
    "    speculative_response = client.chat.completions.create(\n",
    "        model=\"MODEL_NAME,\n",
    "        messages=speculative_prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    return speculative_response.choices[0].message.content\n",
    "\n",
    "# Context Retrieval Function\n",
    "def retrieve_context(question, retriever):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    if not docs:\n",
    "        return \"\"\n",
    "    context = \"Context:\\n\" + \"\\n\".join([f\"Chunk {i + 1}: {doc.page_content}\" for i, doc in enumerate(docs[:3])])\n",
    "    return context\n",
    "\n",
    "# Final Answer Generation\n",
    "def generate_final_answer(question, context):\n",
    "    final_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert in analyzing RFP documents. Use the provided context to generate a complete and detailed answer. \"\n",
    "                       \"If the context does not contain information relevant to the question, respond with 'No Information available'. \"\n",
    "                       \"Ensure the final answer includes at least 7 bullet points when possible.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Context: {context}\\n\\nQuestion: {question}\\n\\nGenerate the Final Answer in bullet points under the heading 'Final Answer from LLM:'.\"\"\"\n",
    "        },\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"MODEL_NAME,\n",
    "        messages=final_prompt,\n",
    "        temperature=0.0,\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36bd7119-8107-473e-b082-c6e0160347e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speculative RAG Pipeline\n",
    "def speculative_rag_pipeline(file_path, question):\n",
    "    # Step 1: Perform speculative generation and context retrieval in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        speculative_future = executor.submit(speculative_generate, question)    \n",
    "        speculative_answer = speculative_future.result()\n",
    "        print(\"speculative_future *************** :\", speculative_answer)\n",
    "        context_future = executor.submit(retrieve_context, question, retriever)\n",
    "        context = context_future.result()\n",
    "        print(\"context_future *************** :\", context)\n",
    "\n",
    "    # Step 2: Validate speculative answer against context\n",
    "    if \"No Information available\" in speculative_answer or not context:\n",
    "        print(\"Speculative answer or context is insufficient. Generating answer directly from context.\")\n",
    "        return generate_final_answer(question, context)\n",
    "\n",
    "    # Step 3: Generate the final answer\n",
    "    final_answer = generate_final_answer(question, context)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d3b17d6-d799-408d-8312-0a5c152359cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speculative_future *************** : \n",
      "Prompt: Create a clear and concise question that encapsulates the following task: Analyze the 'Background' and 'Objectives' sections of a Request for Proposal (RFP) to determine the main challenges faced by the customer. Ensure the question is focused on identifying specific customer needs such as efficiency improvements, technology upgrades, quality enhancement, sustainability, innovation, and compliance.\n",
      "\n",
      "Question: Based on the 'Background' and 'Objectives' sections of the RFP, what are the specific challenges faced by the customer, and what technology or domain-related improvements, enhancements, or innovations are required to address these needs, as indicated by keywords such as 'efficiency improvements', 'technology up\n",
      "context_future *************** : Context:\n",
      "Chunk 1: Carl Zeiss AG \n",
      " \n",
      "Request for Information –  \n",
      "Application Management and Support Provider  \n",
      "(Cloud Integrator) \n",
      " \n",
      "   \n",
      "  \n",
      "   \n",
      "28.07.2023  Page 13 \n",
      " \n",
      "ZEISS classification: Restricted \n",
      "ZEISS classification: Restricted \n",
      "Section 4 – Detailed expectation listing \n",
      "This section contains an overview of the expected components of your proposal which shall be \n",
      "responded to . All documentation shall be provided in English.  ZEISS will conduct th e following \n",
      "process steps (5-phase approach): \n",
      "• RFI (long list) \n",
      "• RFP (short list) \n",
      "• Down select and final negotiations \n",
      "• T&T \n",
      "• Run- Mode \n",
      " \n",
      "Should you be selected for the RFP phase you will be considered as a potential tactical Service \n",
      "Provider in the full ZEISS CIT Multi Provider landscape. This classification of a partnership requires \n",
      "a clear classification in the ZEISS ITSM Ecosystem and its various service providers in the \n",
      "overarching set of rules and regulations.\n",
      "Chunk 2: CIT Corporate IT \n",
      "Cloud \n",
      "Integrator  \n",
      "Definition in relevance to the RFI request: \n",
      "The Cloud Integrator connects various cloud environments, systems  and applications from \n",
      "vendors so that they are fully integrated into the ZEISS IT Service Management and support \n",
      "processes (the so-called ZEISS ITSM Ecosystem). This ensures the interaction of all parties \n",
      "involved in such a way that, in the end, every user within ZEISS is provided with the best \n",
      "possible integrated services based on the ZEISS ITSM Ecosystem.  \n",
      "FMO Future Mode of Operation \n",
      "FWA Framework Agreement \n",
      "GSD Global Service Desk \n",
      "HW Hardware \n",
      "KPI Key Performance Indicator \n",
      "LSD Local Service Desk \n",
      "MIM Major Incident Management \n",
      "PI Performance Indicator \n",
      "Q&A Question and Answer \n",
      "RFI Request for Information \n",
      "RFP Request for Proposal \n",
      "SoW Statement of Work \n",
      "SoW SM Statement of Work Service Management \n",
      "SW Software \n",
      "ZEISS Carl Zeiss AG and subordinated affiliates\n",
      "Chunk 3: a clear classification in the ZEISS ITSM Ecosystem and its various service providers in the \n",
      "overarching set of rules and regulations. \n",
      " \n",
      "For this purpose, ZEISS would already like to give an outlook on the procedure in the RFP phase. \n",
      "ZEISS will then discuss three key contract documents and their annexes together.  \n",
      "• - FWA (default document for all our service providers) \n",
      "• - SoW Service Management (default document for all our service providers)  \n",
      "• - SoW Service (will be individually designed and discussed per service provider) \n",
      " \n",
      "Overview Contract structure:\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "**Final Answer from LLM:**\n",
      "\n",
      "Based on the provided RFP document, the following problem statement can be drafted:\n",
      "\n",
      "1. The current IT landscape at Carl Zeiss AG is facing challenges in integrating various cloud environments, systems, and applications from different vendors into a unified ZEISS IT Service Management Ecosystem.\n",
      "\n",
      "2. There is a need for a clear classification of a partnership with a tactical Service Provider in the ZEISS CIT Multi Provider landscape, adhering to the overarching set of rules and regulations.\n",
      "\n",
      "3. Inefficiencies may arise due to the lack of a streamlined process for connecting and managing multiple cloud environments, leading to potential service disruptions and user dissatisfaction.\n",
      "\n",
      "4. The need for improved service management is indicated by the requirement for detailed contract documents, such as FWA, SoW Service Management, and SoW Service, to ensure seamless integration and operation of services.\n",
      "\n",
      "5. Executive management is seeking innovation and sustainability in the IT infrastructure, necessitating technology upgrades and efficiency improvements.\n",
      "\n",
      "6. Compliance is a concern, as the RFP mentions the need for a partnership that can adhere to ZEISS's classification and service provider regulations within the ZEISS ITSM Ecosystem.\n",
      "\n",
      "7. To address these challenges, Carl Zeiss AG requires a Cloud Integrator that can effectively connect various cloud environments, systems, and applications while ensuring compliance, innovation, and sustainability in their IT infrastructure.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "question = \"\"\"\n",
    "Your question\n",
    "\"\"\"\n",
    "final_answer = speculative_rag_pipeline(file_path, question)\n",
    "\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(final_answer)"
   ]
  },
